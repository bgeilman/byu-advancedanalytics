{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.7/site-packages (21.0.1)\n",
      "Requirement already satisfied: smart-open in /opt/conda/lib/python3.7/site-packages (5.0.0)\n",
      "Requirement already satisfied: flatten_json in /opt/conda/lib/python3.7/site-packages (0.1.13)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from flatten_json) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "# Install exta packages\n",
    "\n",
    "!pip install --upgrade pip\n",
    "!pip install smart-open\n",
    "!pip install flatten_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries and set up variables\n",
    "\n",
    "# import the libraries that we will use\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from smart_open import open\n",
    "from flatten_json import flatten\n",
    "from datetime import datetime\n",
    "\n",
    "# s3 locations\n",
    "bucket = \"emba-final-project\"\n",
    "in_folder = \"training-data\"\n",
    "out_folder = \"csv\"\n",
    "\n",
    "# input test files\n",
    "in_business_file = \"business_test.json\"\n",
    "in_photos_file = \"photos_test.json\"\n",
    "in_review_file = \"review_test.json\"\n",
    "in_user_file = \"user_test.json\"\n",
    "\n",
    "# input train files (override test files)\n",
    "in_business_file = \"business_train.json\"\n",
    "in_photos_file = \"photos_train.json\"\n",
    "in_review_file = \"review_train.json\"\n",
    "in_user_file = \"user_train.json\"\n",
    "\n",
    "# output files\n",
    "out_business_file = \"business_test.csv\"\n",
    "out_photos_file = \"photos_test.csv\"\n",
    "out_review_file = \"review_test.csv\"\n",
    "out_user_file = \"user_test.csv\"\n",
    "\n",
    "# input paths\n",
    "in_business_path = f's3://{bucket}/{in_folder}/{in_business_file}'\n",
    "in_photos_path = f's3://{bucket}/{in_folder}/{in_photos_file}'\n",
    "in_review_path = f's3://{bucket}/{in_folder}/{in_review_file}'\n",
    "in_user_path = f's3://{bucket}/{in_folder}/{in_user_file}'\n",
    "\n",
    "# output paths\n",
    "out_business_path = f's3://{bucket}/{out_folder}/{out_business_file}'\n",
    "out_photos_path = f's3://{bucket}/{out_folder}/{out_photos_file}'\n",
    "out_review_path = f's3://{bucket}/{out_folder}/{out_review_file}'\n",
    "out_user_path = f's3://{bucket}/{out_folder}/{out_user_file}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a few functions\n",
    "\n",
    "# Add a comma to the string passed in if the last character is not a '{'\n",
    "# i.e. add a comma if we are not starting a new entry in the dict\n",
    "def add_comma_if_needed(dict_str):\n",
    "    if not dict_str.endswith('{'):\n",
    "        dict_str = dict_str + ','\n",
    "    return dict_str\n",
    "\n",
    "\n",
    "##############################\n",
    "\n",
    "\n",
    "# Define function to flatten yelp academic JSON data set\n",
    "def fix_dict(data, list_of_lists = [], dict_of_operations = {}):\n",
    "    new_dict_str = \"{\"\n",
    "    \n",
    "    # Create list of keys from dict_of_operations\n",
    "    list_of_dos = []\n",
    "    for do_key, do_value in dict_of_operations.items():\n",
    "        list_of_dos.append(do_key)\n",
    "    \n",
    "    # Process and fix the JSON string data\n",
    "    for key, value in data.items():\n",
    "\n",
    "        # Check key. If its in the list to operate on its values then:\n",
    "        # step 1) get the operation from dict_of_operations\n",
    "        # step 2) do the operation on the values and use the result\n",
    "        #         as a new value for the key\n",
    "        if key in list_of_dos:\n",
    "            new_dict_str = add_comma_if_needed(new_dict_str)\n",
    "            # Get the operation from the dictionary\n",
    "            operation = dict_of_operations.get(key)\n",
    "            # Initialize new_value\n",
    "            new_value = \"\"\n",
    "            # Convert value to list or dict for operation\n",
    "            # Check if string is really a quoted dictionary\n",
    "            if value.startswith('{') and value.endswith('}'):\n",
    "                pass\n",
    "            # Else make it into a list\n",
    "            else:\n",
    "                # Create list from comma delimited string and remove\n",
    "                # leading and trailing white space\n",
    "                new_list = [x.strip() for x in value.split(',')]\n",
    "            # Find and do the operation\n",
    "            if operation == \"count\":\n",
    "                # Count the items shown in the key's value\n",
    "                new_value = len(new_list)\n",
    "                # If there is only one item and it is \"None\" change count to zero\n",
    "                if new_value == 1 and (new_list[0] == \"None\" or new_list[0] == \"\"):\n",
    "                    new_value = 0\n",
    "            elif operation == \"sum\":\n",
    "                # Add the items shown in the key's value\n",
    "                new_value = sum(new_list)\n",
    "            else:\n",
    "                pass\n",
    "            new_dict_str = new_dict_str + '\"' + str(key) + '\":' + str(new_value)\n",
    "            # Go to the next key since we just did special processing for this key\n",
    "            continue\n",
    "\n",
    "        # Check key. If its in the list to convert its values to:\n",
    "        # step 1) a list\n",
    "        # step 2) a dict of booleans with value True\n",
    "        if key in list_of_lists:\n",
    "            new_dict_str = add_comma_if_needed(new_dict_str)\n",
    "            # Create list from comma delimited string and remove\n",
    "            # leading and trailing white space\n",
    "            new_list = [x.strip() for x in value.split(',')]\n",
    "            # print(new_list)\n",
    "            # Start a new dictionary with the key that got us here\n",
    "            new_dict_str = new_dict_str + '\"' + str(key) + '\":{'\n",
    "            # Iterate over the list and add each item as a key\n",
    "            # with a value of True\n",
    "            for i in new_list:\n",
    "                new_dict_str = add_comma_if_needed(new_dict_str)\n",
    "                new_dict_str = new_dict_str + '\"' + str(i) + '\":' + str(True)\n",
    "            # end the dictionary\n",
    "            new_dict_str = new_dict_str + '}'\n",
    "            # Go to the next key since we just did special processing for this key\n",
    "            continue\n",
    "\n",
    "        # Add int and float entries as is without qoutes\n",
    "        if isinstance(value, (int, float)):\n",
    "            new_dict_str = add_comma_if_needed(new_dict_str)\n",
    "            new_dict_str = new_dict_str + '\"' + str(key) + '\":' + str(value)\n",
    "\n",
    "        # Fix any nested dictionary entries and add them\n",
    "        if isinstance(value, (dict)):\n",
    "            new_dict_str = add_comma_if_needed(new_dict_str)\n",
    "            new_dict_str = new_dict_str + '\"' + str(key) + '\":' + fix_dict(value)\n",
    "\n",
    "        # Add list entries as is without quotes\n",
    "        # (kept this separate from int and float in case future special processing is needed)\n",
    "        if isinstance(value, (list)):\n",
    "            new_dict_str = add_comma_if_needed(new_dict_str)\n",
    "            new_dict_str = new_dict_str + '\"' + str(key) + '\":' + str(value)\n",
    "\n",
    "        # Some strings may actually be a dictionary in quotes\n",
    "        # If it's a quoted dictionary, remove the quotes, fix, and keep it nested in place\n",
    "        # Otherwise, just add the string as is\n",
    "        if isinstance(value, (str)):\n",
    "            new_dict_str = add_comma_if_needed(new_dict_str)\n",
    "            # Check if string is really a quoted dictionary\n",
    "            if value.startswith('{') and value.endswith('}'):\n",
    "                value = eval(value)\n",
    "                new_dict_str = new_dict_str + '\"' + str(key) + '\":' + fix_dict(value)\n",
    "            # if not, just add it as is\n",
    "            else:\n",
    "                new_dict_str = new_dict_str + '\"' + str(key) + '\":\"' + str(value) + '\"'\n",
    "\n",
    "    # Close the dictionary \n",
    "    new_dict_str = new_dict_str + '}'\n",
    "    # and return fixed dictionary (as a string--it gets converted to a dict object upon return)\n",
    "    return new_dict_str\n",
    "\n",
    "\n",
    "##############################\n",
    "\n",
    "\n",
    "def json_string_to_flat_df(line, special_keys = [], operation_keys = {}):\n",
    "\n",
    "    # Replace \"null\" strings with empty strings\n",
    "    line = line.replace(\"null\", \"\\\"\\\"\")\n",
    "    # Replace escaped double quotes with a single quote\n",
    "    line = line.replace(\"\\\\\\\"\", \"'\")\n",
    "    # Escape any backslashes with another backslash\n",
    "    line = line.replace(\"\\\\\", \"\\\\\\\\\")\n",
    "    \n",
    "    # print(line)\n",
    "    \n",
    "    # Change string to a dictionary object\n",
    "    ydict = eval(line)\n",
    "    # print(ydict)\n",
    "    \n",
    "    # Fix stingified dictionaries and change big string of values into\n",
    "    # into dictionary of booleans\n",
    "    better_ydict = eval(fix_dict(ydict, special_keys, operation_keys))\n",
    "    \n",
    "    # Flatten the dictionary to be added to a DataFrame \n",
    "    ydict_flattened = pd.json_normalize(better_ydict)\n",
    "    \n",
    "    return ydict_flattened\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process business data\n",
    "\n",
    "# Initialize DataFrame\n",
    "bdf = pd.DataFrame([])\n",
    "\n",
    "# Read in the data a line at a time\n",
    "for line in open(in_business_path, encoding='utf8'):\n",
    "    # print(line)\n",
    "    \n",
    "    # Flatten and fix json string to be added to DataFram\n",
    "    flat_df = json_string_to_flat_df(line, ['categories'])\n",
    "    \n",
    "    # Add the flattened dictionary to a DataFrame\n",
    "    bdf = bdf.append(flat_df, ignore_index=True)\n",
    "\n",
    "    \n",
    "# Eliminate unwanted columns\n",
    "bdf.drop( \\\n",
    "         ['address', \\\n",
    "          'latitude', \\\n",
    "          'longitude', \\\n",
    "          'is_open', \\\n",
    "          'hours', \\\n",
    "          'hours.Monday', \\\n",
    "          'hours.Tuesday', \\\n",
    "          'hours.Wednesday', \\\n",
    "          'hours.Thursday', \\\n",
    "          'hours.Friday', \\\n",
    "          'hours.Saturday', \\\n",
    "          'hours.Sunday'], \\\n",
    "         axis=1, \\\n",
    "         inplace=True \\\n",
    "        )    \n",
    "\n",
    "# Save the DataFrame to CSV files, both locally (for easy inspection)\n",
    "# and to an S3 bucket\n",
    "bdf.to_csv(out_business_file) # local\n",
    "bdf.to_csv(out_business_path) # S3\n",
    "\n",
    "# Review DataFrame\n",
    "bdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process photo data\n",
    "\n",
    "# Initialize DataFrame\n",
    "pdf = pd.DataFrame([])\n",
    "\n",
    "# Read in the data a line at a time\n",
    "for line in open(in_photos_path, encoding='utf8'):\n",
    "    # print(line)\n",
    "    \n",
    "    # Flatten and fix json string to be added to DataFram\n",
    "    flat_df = json_string_to_flat_df(line)\n",
    "    \n",
    "    # Add the flattened dictionary to a DataFrame\n",
    "    pdf = pdf.append(flat_df, ignore_index=True)\n",
    "\n",
    "\n",
    "# Save the DataFrame to CSV files, both locally (for easy inspection)\n",
    "# and to an S3 bucket\n",
    "pdf.to_csv(out_photos_file) # local\n",
    "pdf.to_csv(out_photos_path) # S3\n",
    "\n",
    "# Review DataFrame\n",
    "pdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process review data\n",
    "\n",
    "# Initialize DataFrame\n",
    "rdf = pd.DataFrame([])\n",
    "\n",
    "# Read in the data a line at a time\n",
    "for line in open(in_review_path, encoding='utf8'):\n",
    "    # print(line)\n",
    "    \n",
    "    # Flatten and fix json string to be added to DataFram\n",
    "    flat_df = json_string_to_flat_df(line)\n",
    "    \n",
    "    # Add the flattened dictionary to a DataFrame\n",
    "    rdf = rdf.append(flat_df, ignore_index=True)\n",
    "\n",
    "\n",
    "# Add individual date and time features from existing datetime feature\n",
    "now = datetime.now()\n",
    "rdf['dt_obj'] = pd.to_datetime(rdf['date'], format='%Y-%m-%d %H:%M:%S')\n",
    "rdf['year'] = rdf['dt_obj'].dt.year\n",
    "rdf['month'] = rdf['dt_obj'].dt.month\n",
    "rdf['hour'] = rdf['dt_obj'].dt.hour\n",
    "rdf['review_age'] = now - rdf.dt_obj\n",
    "rdf['review_age'] = rdf.review_age / np.timedelta64(1, 'Y')\n",
    "\n",
    "\n",
    "# Save the DataFrame to CSV files, both locally (for easy inspection)\n",
    "# and to an S3 bucket\n",
    "rdf.to_csv(out_review_file) # local\n",
    "rdf.to_csv(out_review_path) # S3\n",
    "\n",
    "# Review DataFrame\n",
    "rdf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process user data\n",
    "\n",
    "# Initialize DataFrame\n",
    "udf = pd.DataFrame([])\n",
    "\n",
    "# Read in the data a line at a time\n",
    "for line in open(in_user_path, encoding='utf8'):\n",
    "    # print(line)\n",
    "    \n",
    "    # Flatten and fix json string to be added to DataFram\n",
    "    flat_df = json_string_to_flat_df(line, [], {\"friends\":\"count\",\"elite\":\"count\"})\n",
    "    \n",
    "    # Add the flattened dictionary to a DataFrame\n",
    "    udf = udf.append(flat_df, ignore_index=True)\n",
    "\n",
    "\n",
    "# Add individual date and time features from existing datetime feature\n",
    "now = datetime.now()\n",
    "udf['dt_obj'] = pd.to_datetime(udf['yelping_since'], format='%Y-%m-%d %H:%M:%S')\n",
    "udf['yelp_age'] = now - udf.dt_obj\n",
    "udf['yelp_age'] = udf.yelp_age / np.timedelta64(1, 'Y')\n",
    "\n",
    "\n",
    "# Save the DataFrame to CSV files, both locally (for easy inspection)\n",
    "# and to an S3 bucket\n",
    "udf.to_csv(out_user_file) # local\n",
    "udf.to_csv(out_user_path) # S3\n",
    "\n",
    "# Review DataFrame\n",
    "udf.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
